{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import r2_score\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as mpl\n",
    "from random import randint, choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Load data, and simplify/clean it. This includes: only selecting variables needed; encoding the species; dropping NaN data in both censuses; converting to numpy arrays; altering such that labels represent growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zf/kbwgwbdd4gjfhdxc9cnw8qgm0000gn/T/ipykernel_16875/666747286.py:1: DtypeWarning: Columns (14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  t1 = pd.read_csv(\"/Users/dylanvanbramer/indresearch/xu/deep_learning/bci_census/bci_1985.csv\")\n",
      "/var/folders/zf/kbwgwbdd4gjfhdxc9cnw8qgm0000gn/T/ipykernel_16875/666747286.py:2: DtypeWarning: Columns (14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  t2 = pd.read_csv(\"/Users/dylanvanbramer/indresearch/xu/deep_learning/bci_census/bci_1990.csv\")\n"
     ]
    }
   ],
   "source": [
    "t1 = pd.read_csv(\"/Users/dylanvanbramer/indresearch/xu/deep_learning/bci_census/bci_1985.csv\")\n",
    "t2 = pd.read_csv(\"/Users/dylanvanbramer/indresearch/xu/deep_learning/bci_census/bci_1990.csv\")\n",
    "\n",
    "# COMMENT OUT IF USING ALL OF THE DATA\n",
    "t1 = t1.loc[t1['quadrat']<=500]\n",
    "t2 = t2.loc[t2['quadrat']<=500]\n",
    "\n",
    "# simplify columns and column names\n",
    "expected_labels = t2[['treeID', 'dbh']]\n",
    "expected_labels = expected_labels.rename(columns={\"dbh\": \"dbh2\", \"treeID\": \"treeID2\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expected_labels.dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurest1 = t1[['treeID', 'sp', 'dbh', 'gx', 'gy']]\n",
    "featurest1 = featurest1.rename(columns={\"dbh\": \"dbh1\", \"treeID\":\"treeID1\"})\n",
    "\n",
    "sp_list = featurest1.sp.unique()\n",
    "sp_num = sp_list.shape[0]\n",
    "a = sp_num\n",
    "\n",
    "# encode the species into binary features and drop rows with any NaN values\n",
    "encoder= ce.OneHotEncoder(cols=['sp'],return_df=True)\n",
    "featurest1 = encoder.fit_transform(featurest1)\n",
    "\n",
    "df_combined = pd.concat([featurest1, expected_labels], axis=1)\n",
    "df_combined_clean = df_combined.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# split back up and convert the dataframes to numpy arrays\n",
    "df_clean = df_combined_clean[featurest1.columns]\n",
    "labels_clean = df_combined_clean[expected_labels.columns]\n",
    "\n",
    "featurest1 = df_clean.to_numpy()\n",
    "expected_labels = labels_clean.to_numpy()\n",
    "\n",
    "featurest1 = featurest1.astype(np.float32)\n",
    "expected_labels = expected_labels.astype(np.float32)\n",
    "\n",
    "\n",
    "# change from numpy using round and then multiply by 5 (np a round)\n",
    "#featurest1[:,10] = np.around(featurest1[:,10]/5)* 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df_combined_clean['treeID1']-df_combined_clean['treeID2']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Edit variable \"a\" to represent how many columns the species encoding uses. This will impact calculations further in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that there is only positive growth, represented by CHANGE (not total dbh)\n",
    "expected_labels[:,1] = expected_labels[:,1] - featurest1[:,a+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_labels = np.where (expected_labels<0, 0, expected_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***INTRINSIC MODEL BELOW***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_in, X_test_in, y_train_in, y_test_in = train_test_split(featurest1, expected_labels, test_size=0.3)\n",
    "\n",
    "train_feats_in = X_train_in[:,1:a+2]\n",
    "train_labels_in = y_train_in[:,1]\n",
    "\n",
    "test_labels_in = y_test_in[:,1]\n",
    "test_feats_in = X_test_in[:,1:a+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15386022451209003\n"
     ]
    }
   ],
   "source": [
    "rf_in = RandomForestRegressor(max_depth=3)\n",
    "rf_in.fit(train_feats_in[:,1:], train_labels_in)\n",
    "preds_in = rf_in.predict(test_feats_in[:,1:])\n",
    "error_in = r2_score(test_labels_in, preds_in)\n",
    "print (error_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_in.feature_importances_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out of bag score for random forests (some samples it never sees bc of resampling) - proxy for validation set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Separate out the x coordinates and y coordinates. Create a matrix with the distance from each focal tree to its nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = featurest1[:,0]\n",
    "# X WOULD BE a +1, y would be a+2\n",
    "x_coordinates = featurest1[:, a+2]  \n",
    "y_coordinates = featurest1[:, a+3]\n",
    "coord_matrix = np.column_stack((x_coordinates, y_coordinates))\n",
    "spatial_tree = sp.spatial.KDTree(coord_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Create \"blanks,\" or accumulators for the nearest neighbors to accumulate themselves in. Though we are looking at the 5 nearest neighbors, include 6 columns because the first closest neighbor will be itself. We will later remove this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dist_matrix2 = np.zeros((len(coord_matrix),21))\n",
    "nn_ind_matrix2 = np.zeros((len(coord_matrix),21))\n",
    "nn_feats = featurest1[:, 0:a+2]\n",
    "#nn_feats = np.column_stack((featurest1[:, 0:a+1], featurest1[:,a+3]))\n",
    "#species, dbh\n",
    "feats_matrix = np.zeros((len(coord_matrix),(21*(a+2))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Loop through the coordinate matrix, and add the features of the neighbors to a large data structure. In this data structure, each row represents ONE tree. The first  features of this row are the focal tree's characteristics. The features further to the right are the features of the neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tree in enumerate(coord_matrix):\n",
    "    dist2, ind2 = spatial_tree.query(tree, k=21)\n",
    "    nn_ind_matrix2[i] = ids[ind2]\n",
    "    nn_dist_matrix2[i]= dist2\n",
    "\n",
    "    nn_row = nn_feats[i].reshape(1,a+2)\n",
    "    inc = 0\n",
    "    for j in nn_ind_matrix2[i][1:]:\n",
    "        row_ind = np.where(featurest1[:,0] == j)\n",
    "        real_row = (featurest1[row_ind])\n",
    "        distance = dist2[1:][inc].reshape(1,1)\n",
    "        dbh = real_row[:,a+1].reshape(1,1)\n",
    "        nn_row = np.hstack((nn_row, distance, real_row[:,1:a+1],dbh))\n",
    "        inc += 1\n",
    "    \n",
    "    feats_matrix[i] = nn_row\n",
    "    \n",
    "#nn_ind_matrix2 = nn_ind_matrix2[:,1:]\n",
    "#nn_dist_matrix2 = nn_dist_matrix2[:,1:]\n",
    "# remove the columns with the focal tree's OWN distance and IDs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Split into test and train data using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feats_matrix, expected_labels, test_size=0.3)\n",
    "\n",
    "train_feats = X_train[:,1:]\n",
    "train_labels = y_train[:,1]\n",
    "\n",
    "test_labels = y_test[:,1]\n",
    "test_feats = X_test[:,1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Do a hyperparameter search for ideal hyperparameters for random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER SEARCH\n",
    "n_estimators = [100,200,400]\n",
    "max_depth = [7,8,9]\n",
    "random_grid = {'max_depth': max_depth, 'n_estimators': n_estimators}\n",
    "rf = RandomForestRegressor()\n",
    "rf_grid = GridSearchCV(rf, param_grid= random_grid, scoring='r2', cv=5 )\n",
    "rf_grid.fit(train_feats[:,1:], train_labels)\n",
    "depth = rf_grid.best_params_['max_depth']\n",
    "est = rf_grid.best_params_['n_estimators']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with continuous value species indexed in dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bar chart of feature importance by name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Complete training and testing using these hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1427227129747809\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(max_depth=5, n_jobs = -1, n_estimators=100)\n",
    "rf.fit(train_feats[:,1:], train_labels)\n",
    "preds = rf.predict(test_feats[:,1:])\n",
    "error1a = r2_score(test_labels, preds)\n",
    "print (error1a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a dataframe with column names including each species encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) **Begin to introduce randomization**: Create a matrix where each row is a different permutation of the series [1,2,3,4,5]. Here, we do this 20 times (ie: 20 rows, 20 randomizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 10 random sequences of 1 to 5 - COMPLETE\n",
    "rng = np.random.default_rng()\n",
    "matrix_order = np.zeros((10,20))\n",
    "\n",
    "#np.random.permutation\n",
    "\n",
    "shuffle_this = np.array(range(20))\n",
    "for i in range(10):\n",
    "  current_row = rng.permutation(shuffle_this)\n",
    "  matrix_order[i] = current_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_order = matrix_order.astype(int)\n",
    "matrix_order.shape\n",
    "#10 permutations of 20 neighbors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Again, create sort of \"blanks,\" or accumulators for the loop to use later on. These will eventually become our larger, randomized data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dist_matrix3 = np.zeros((len(coord_matrix),21))\n",
    "nn_ind_matrix3 = np.zeros((len(coord_matrix),21))\n",
    "nn_feats2 = featurest1[:, 0:a+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a+2\n",
    "#stack into one list\n",
    "\n",
    "zer_indices = list(range(b, 2*b))\n",
    "one_indices = list(range(2*b, 3*b))\n",
    "two_indices = list(range(3*b, 4*b))\n",
    "three_indices = list(range(4*b, 5*b))\n",
    "four_indices = list(range(5*b, 6*b))\n",
    "five_indices = list(range(6*b, 7*b))\n",
    "six_indices = list(range(7*b, 8*b))\n",
    "seven_indices = list(range(8*b, 9*b))\n",
    "eight_indices = list(range(9*b, 10*b))\n",
    "nine_indices = list(range(10*b, 11*b))\n",
    "ten_indices = list(range(11*b, 12*b))\n",
    "eleven_indices = list(range(12*b, 13*b))\n",
    "twelve_indices = list(range(13*b, 14*b))\n",
    "thirt_indices = list(range(14*b, 15*b))\n",
    "fourt_indices = list(range(15*b, 16*b))\n",
    "fift_indices = list(range(16*b, 17*b))\n",
    "sixt_indices = list(range(17*b, 18*b))\n",
    "sevent_indices = list(range(18*b, 19*b))\n",
    "eighteen_indices = list(range(19*b, 20*b))\n",
    "nineteen_indices = list(range(20*b, 21*b))\n",
    "\n",
    "\n",
    "indices = np.array([\n",
    "   zer_indices, one_indices, two_indices, three_indices, four_indices, five_indices, six_indices,\n",
    "   seven_indices, eight_indices, nine_indices, ten_indices, eleven_indices, \n",
    "   twelve_indices, thirt_indices, fourt_indices, fift_indices, sixt_indices,\n",
    "     sevent_indices, eighteen_indices, nineteen_indices\n",
    "])\n",
    "\n",
    "foobar = indices[matrix_order]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cos and sin of angle for position embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Iterate through those data structures, and create the correct, randomized structure. This way, the model will learn to use the DISTANCE associated with each tree, not just its index. (Sort of creating synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with feats_matrix above. Then create similar matrices for each of the above\n",
    "feats_matrix_big = feats_matrix.copy()\n",
    "\n",
    "# # the long number passing into range is ust n=20 here, but this would make it less hard-coded in\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    # loop through each permutation.\n",
    "    feats_matrix_small = feats_matrix.copy()\n",
    "    \n",
    "   \n",
    "    flattened = []\n",
    "   # loop through each neighbor (each row)\n",
    "    for j in range(foobar[i,:,:].shape[0]):\n",
    "       #for item in sublist:\n",
    "       for foo in foobar[i,j,:]:\n",
    "           flattened.append(foo)\n",
    "           #flattened.append(foobar[i,j,:])\n",
    "       #flattened.append(foobar[i,j,:])\n",
    "    \n",
    "    flattened = list(range(a+2))+flattened\n",
    "    \n",
    "   #  for sublist in perm_indices:\n",
    "   #    \n",
    "    feats_matrix_small = feats_matrix_small[:,flattened]\n",
    "    feats_matrix_big = np.vstack((feats_matrix_big,feats_matrix_small))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Create a data structure to store the corresponding y labels. Essentially, just vertically append them to each other, the same number of times as permutations we have above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  - THIS WORKS! # of rows = 138x20\n",
    "expected_labels2 = expected_labels.copy()\n",
    "for i in range(10):\n",
    "    expected_labels2 = np.vstack((expected_labels2, expected_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) Train a new random forest regressor on this? Or use old one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(feats_matrix_big, expected_labels2, test_size=0.3)\n",
    "\n",
    "train_feats2 = X_train2[:,1:]\n",
    "train_labels2 = y_train2[:,1]\n",
    "\n",
    "test_labels2 = y_test2[:,1]\n",
    "test_feats2 = X_test2[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER SEARCH\n",
    "n_estimators = [100,200,400]\n",
    "max_depth = [7,8,9]\n",
    "random_grid = {'max_depth': max_depth, 'n_estimators': n_estimators}\n",
    "rf_big = RandomForestRegressor()\n",
    "rf_grid_big = GridSearchCV(rf, param_grid= random_grid, scoring='r2', cv=5 )\n",
    "rf_grid_big.fit(train_feats2[:,1:], train_labels2)\n",
    "depth2 = rf_grid_big.best_params_['max_depth']\n",
    "est2 = rf_grid_big.best_params_['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1932328554132413\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(max_depth=3, n_estimators=100)\n",
    "rf.fit(train_feats2[:,1:], train_labels2)\n",
    "preds2 = rf.predict(test_feats2[:,1:])\n",
    "error2 = r2_score(test_labels2, preds2)\n",
    "print (error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shoould try on running  anew one as well as using the rf i made without permutations?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15) Use framework from diagnostics file to create a sorted list of species by decreasing abundance. This will allow us to call on certain species for testing, plotting, and printing. This is something that could be made into its own file in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = encoder.inverse_transform(featurest1)\n",
    "sp_list = names.sp.unique()\n",
    "sp_num = sp_list.shape[0]\n",
    "sp_freq = names.sp.value_counts(sort=True).index.tolist()\n",
    "sp_num = sp_list.shape[0]\n",
    "#sp_mat = np.hstack((np.atleast_2d(sp_list).transpose(), (np.zeros((sp_num, a)))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16) Create a data structure where each species tree has a DBH in the range 10-500, every 5 mm. This is \"one_big_array.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dbhs = np.arange(10,501,5).transpose()\n",
    "all_dbhs_big = np.arange(10,501,5).transpose()\n",
    "list_of_all_sp = []\n",
    "\n",
    "for i in range(sp_num):\n",
    "    # 1) look at initial dataset and find the tree with that ID, then find its encoding\n",
    "    # find tree ID of one instance of a species\n",
    "    tree_id = names[names.sp == sp_freq[i]].treeID1\n",
    "    one_tree_id = tree_id.iloc[0]\n",
    "    encoding = feats_matrix_big[feats_matrix_big[:,0]==one_tree_id][0,1:a+1]\n",
    "    sp_label = np.atleast_2d(encoding)\n",
    "    sp_repeated = np.repeat(sp_label,99,axis=0)\n",
    "    sp_i = np.column_stack((sp_repeated,all_dbhs))\n",
    "    all_dbhs_big = np.vstack((all_dbhs_big, all_dbhs))\n",
    "    list_of_all_sp.append(sp_i)\n",
    "\n",
    "one_big_array = np.vstack(list_of_all_sp)\n",
    "all_dbhs_big = (all_dbhs_big.flatten())[:a*3234] # can be represented by a*3234?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17) Next, run these hallucinated trees through our trained model (either the intrinsic model, or through the neighborhood model with zero neighbors and lots of padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_diagnostic = rf_in.predict(one_big_array[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1_preds = preds_diagnostic[:99]\n",
    "sp2_preds = preds_diagnostic[99:198]\n",
    "sp3_preds = preds_diagnostic[198:297]\n",
    "sp4_preds = preds_diagnostic[297:396]\n",
    "sp5_preds = preds_diagnostic[396:495]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.scatter(all_dbhs,sp1_preds, label = \"sp1\")\n",
    "mpl.scatter(all_dbhs,sp2_preds, label = \"sp2\")\n",
    "mpl.scatter(all_dbhs,sp3_preds, label = \"sp3\")\n",
    "mpl.scatter(all_dbhs,sp4_preds, label = \"sp4\")\n",
    "mpl.scatter(all_dbhs,sp5_preds, label = \"sp5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_of_all_preds = []\n",
    "for sp in list_of_all_sp:\n",
    "    # adjust as if it had all zero neighbors\n",
    "    #sp_adjusted = np.column_stack((sp,np.zeros((99,5*(a+2)))))\n",
    "    preds_diagnostic = rf_in.predict(sp)\n",
    "    list_of_all_preds.append(preds_diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.nn.Embedding(sp_num,8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graph # neighbors vs. r2, gaussian process regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xu-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, Oct 26 2021, 07:25:54) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74ea42a2acbac2542a5ba28118d5ac45aa30cdd1086d6bdd4fc2eb17ce0fd824"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
